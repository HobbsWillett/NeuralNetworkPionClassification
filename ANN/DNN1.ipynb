{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Version , oversampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 7.0087080001831055 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import time\n",
    "start_time = time.time()\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.metrics import binary_accuracy\n",
    "#from keras.utils import np_utils\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2.4018337726593018 seconds ---\n",
      "(46280, 21)\n",
      " FirstLayer  LastLayer NHits  AverageZP Thrust  PID_Angle  PID_Front  PID_LLR_M  FirstLayer  LastLayer  NHits_Low  AverageZP  Thrust_Lo  PID_Angle  PID_Front  PID_LLR_M  Energy_As  Angle_Bet  Distance_Bet   Sig   Bg\n"
     ]
    }
   ],
   "source": [
    "# import datasets with time taken!\n",
    "#smoll\n",
    "\"\"\" # commented out to save computation\n",
    "start_time = time.time()\n",
    "smoll = np.loadtxt(\"/home/willett/NeutrinoData/small_CNN_input_processed.txt\", comments='#')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(smoll.shape)\n",
    "\"\"\"\n",
    "\n",
    "#and the full\n",
    "start_time = time.time()\n",
    "fll = np.loadtxt(\"/home/willett/NeutrinoData/full_CNN_input_processed.txt\", comments='#')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(fll.shape)\n",
    "\n",
    "\n",
    "\"\"\" # commented out to save computation\n",
    "#and the full\n",
    "start_time = time.time()\n",
    "fll = np.loadtxt(\"/home/willett/NeutrinoData/test_CNN_input_processed.txt\", comments='#')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(fll.shape)\n",
    "\"\"\"\n",
    "\n",
    "# extract title\n",
    "pls = open(\"/home/willett/NeutrinoData/small_CNN_input_processed.txt\", \"r\")\n",
    "title = pls.readline()\n",
    "title = title[2:-1]\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  (46280, 21) \n",
      "size:  971880  \n",
      "length:  46280\n"
     ]
    }
   ],
   "source": [
    "# creating a dataset switch, change what UsedData is to change CNN\n",
    "UD = fll # Used Data = <dataset>\n",
    "UDLength = UD.shape[0]\n",
    "print(\"shape: \",UD.shape,\"\\nsize: \", UD.size,\" \\nlength: \", UDLength)\n",
    "\n",
    "# dataset is expected in this format:\n",
    "# FirstLayer  LastLayer NHits  AverageZP Thrust  PID_Angle  PID_Front  PID_LLR_M\n",
    "#FirstLayer  LastLayer  NHits_Low  AverageZP  Thrust_Lo  PID_Angle  PID_Front  PID_LLR_M  \n",
    "#Energy_As  Angle_Bet  Distance_Bet   Sig   Bg\n",
    "\n",
    "# with Sig and Bg expected as one hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (46280, 19) \n",
      "Y shape:  (46280, 2)\n"
     ]
    }
   ],
   "source": [
    "# splitting X = dataset , Y = one hot vectors\n",
    "X = UD[:,0:-2]\n",
    "Y = UD[:,-2:1000]\n",
    "print(\"X shape: \",X.shape,\"\\nY shape: \", Y.shape)\n",
    "\n",
    "# they will be split into testing and training at compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " signal and background event number:  1148 45132 \n",
      " number more needed: 39\n",
      "initial Signal to Noise ratio:  2.4805531547104582 % signal\n",
      "(44772, 19) (44772, 2)\n",
      "(89904, 19) (89904, 2)\n",
      "these arrays should be different vertically, to ensure shuffle succesful:\n",
      "[0.00829848 0.0039091  0.00049919] [0. 0. 0.]\n",
      "[0.00053444 0.00076204 0.00294091] [0. 1. 0.]\n",
      "final Signal Noise ratio:  49.799786438868125 % signal\n",
      "--- 0.3482367992401123 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# inevitable bias removal... by oversampling\n",
    "# using a 50% oversampling ratio, because i want to ! (no citation)\n",
    "\n",
    "#how long?\n",
    "start_time = time.time()\n",
    "\n",
    "SigI = np.where(Y[:,0] == 1)[0] \n",
    "BgI= np.where(Y[:,0] == 0)[0]\n",
    "SigN = SigI.size       # how much signal there is\n",
    "BgN = BgI.size         # how much background there is\n",
    "Multip = int(BgN/SigN) # how much more signal event copies needed for ~50%\n",
    "print(\" signal and background event number: \",SigI.size,BgI.size,\"\\n number more needed:\",Multip)\n",
    "SNratio =  (100*SigN)/(SigN + BgN)\n",
    "print(\"initial Signal to Noise ratio: \",SNratio,\"% signal\")\n",
    "\n",
    "#im going to reconstruct the arrays of signal events, background events, then add them together and shuffle!\n",
    "\n",
    "XSig = X[SigI]\n",
    "XBg = X[BgI]\n",
    "YSig = Y[SigI]\n",
    "YBg = Y[BgI]\n",
    "#print(XSig.shape,XBg.shape, YSig.shape, YBg.shape) # these are the events of each type.\n",
    "\n",
    "# this is the array of signal repreated (tiled) multip times.\n",
    "YSigM = np.transpose(np.tile(np.transpose(YSig), Multip))\n",
    "XSigM = np.transpose(np.tile(np.transpose(XSig), Multip)) \n",
    "print( XSigM.shape, YSigM.shape)\n",
    "\n",
    "#adding arrays together and then shuffling:\n",
    "X2 = np.concatenate((XBg,XSigM))\n",
    "Y2 = np.concatenate((YBg,YSigM))\n",
    "print(X2.shape, Y2.shape)\n",
    "\n",
    "#shuffling\n",
    "print(\"these arrays should be different vertically, to ensure shuffle succesful:\")\n",
    "print(X2[0:3,0],Y2[0:3,0])\n",
    "np.random.shuffle(X2)\n",
    "np.random.shuffle(Y2)\n",
    "print(X2[0:3,0],Y2[0:3,0])\n",
    "\n",
    "#final ratio:\n",
    "NewSigN = YSigM.shape[0]\n",
    "SNRatioNew = (100*YSigM.shape[0]) / (YSigM.shape[0] + BgN)\n",
    "print(\"final Signal Noise ratio: \",SNRatioNew,\"% signal\")\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89904, 19)\n",
      "(89904, 19, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X2.shape)\n",
    "X3 = np.expand_dims(X2, axis=2)\n",
    "print(X3.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.4803647994995117 seconds ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#neural network architecture:\n",
    "model = Sequential()\n",
    "\n",
    "# set variables:\n",
    "width = 30 #--number on nodes in the layer\n",
    "DR = 0.5 #--fraction of nodes dropped during training\n",
    "AT = \"sigmoid\" #--activation type for dense layers\n",
    "UB = True #--use bias vectors \n",
    "InDim = (X3.shape[1],X3.shape[2] ) #--input shape of single sample (tuple)\n",
    "\n",
    "#construction:\n",
    "start_time = time.time() # how long does it take?\n",
    "model.add(Dense(width,activation=AT, use_bias=UB, input_shape=(19,1) )) # input layer and 1\n",
    "model.add(Dropout(DR))\n",
    "model.add(Dense(width,activation=AT, use_bias=UB )) # 2\n",
    "model.add(Dropout(DR))\n",
    "model.add(Dense(width,activation=AT, use_bias=UB )) # 3\n",
    "model.add(Dropout(DR))\n",
    "model.add(Dense(width,activation=AT, use_bias=UB )) # 4\n",
    "model.add(Dropout(DR))\n",
    "\n",
    "#Because this one is deep:\n",
    "model.add(Dense(width,activation=AT, use_bias=UB )) # 3\n",
    "model.add(Dropout(DR))\n",
    "model.add(Dense(width,activation=AT, use_bias=UB )) # 4\n",
    "model.add(Dropout(DR))\n",
    "model.add(Dense(width,activation=AT, use_bias=UB )) # 3\n",
    "model.add(Dropout(DR))\n",
    "model.add(Dense(width,activation=AT, use_bias=UB )) # 4\n",
    "model.add(Dropout(DR))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Flatten())  # reduce dimensionality of the input data for output\n",
    "\n",
    "model.add(Dense(2, activation=\"softmax\", use_bias=UB)) # output layer softmax recommended \n",
    "#(classification mutually excluive + softmax differentiable for optimizing)\n",
    "# -> https://www.quora.com/Artificial-Neural-Networks-Why-do-we-use-softmax-function-for-output-layer\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply regularizer if overfitting! ^\n",
    "\n",
    "# binary_crossentropy is the best according to https://www.dlology.com/blog/how-to-choose-last-layer-activation-and-loss-function/\n",
    "# adam is best for me according to https://towardsdatascience.com/types-of-optimization-algorithms-used-in-neural-networks-and-ways-to-optimize-gradient-95ae5d39529f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model:\n",
    "start_time = time.time() # how long does it take?\n",
    "model.compile(optimizer='adagrad',            \n",
    "              loss='binary_crossentropy',               \n",
    "              metrics=['accuracy', 'binary_accuracy' ])\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "\n",
    "start_time = time.time()             # how long does it take?\n",
    "history = model.fit(X3,              # the (now oversampled) dataset\n",
    "          Y2,                        #true or false values for the dataset \n",
    "          epochs=100,                 #number of iteration over data\n",
    "          batch_size=32,             #number of trainings between tests\n",
    "          verbose=1,                 #prints one line per epoch of progress bar\n",
    "          validation_split=0.1 )     #ratio of test to train\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summarise history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
